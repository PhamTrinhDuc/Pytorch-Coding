{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipeline](image.png)\n",
    "\n",
    "\n",
    "combine embedding của question và context -> model -> phân loại trong các câu trả lời "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset = [\n",
    "    {\n",
    "        'context': 'My name is AIVN and I am from Vietnam.',\n",
    "        'question': 'What is my name?',\n",
    "        'answer': 'AIVN'\n",
    "    },\n",
    "    {\n",
    "        'context': 'I love painting and my favorite artist is Vincent Van Gogh.',\n",
    "        'question': 'What is my favorite activity?',\n",
    "        'answer': 'painting'\n",
    "    },\n",
    "    {\n",
    "        'context': 'I am studying computer science at the University of Tokyo.',\n",
    "        'question': 'What am I studying?',\n",
    "        'answer': 'computer science'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ducpham/anaconda3/lib/python3.11/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/ducpham/anaconda3/lib/python3.11/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/ducpham/anaconda3/lib/python3.11/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vincent': 32,\n",
       " 'vietnam': 31,\n",
       " 'van': 30,\n",
       " 'university': 29,\n",
       " 'what': 11,\n",
       " '<sep>': 4,\n",
       " '<bos>': 2,\n",
       " 'of': 24,\n",
       " 'am': 10,\n",
       " 'my': 7,\n",
       " 'is': 6,\n",
       " 'at': 19,\n",
       " '.': 8,\n",
       " 'gogh': 22,\n",
       " '<eos>': 3,\n",
       " '<pad>': 1,\n",
       " 'computer': 20,\n",
       " 'painting': 25,\n",
       " 'and': 12,\n",
       " '<unk>': 0,\n",
       " 'artist': 18,\n",
       " 'favorite': 13,\n",
       " 'studying': 15,\n",
       " 'i': 5,\n",
       " 'aivn': 17,\n",
       " 'tokyo': 28,\n",
       " 'name': 14,\n",
       " 'activity': 16,\n",
       " 'from': 21,\n",
       " 'love': 23,\n",
       " '?': 9,\n",
       " 'science': 26,\n",
       " 'the': 27}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "tokenier = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(examples: list):\n",
    "    for item in examples:\n",
    "        yield tokenier(item[\"context\"] + \" \" + item[\"question\"])\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    iterator=yield_tokens(qa_dataset), \n",
    "    specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\", \"<sep>\"]\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 'painting', 1: 'AIVN', 2: 'computer science'},\n",
       " {'painting': 0, 'AIVN': 1, 'computer science': 2})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = set([items[\"answer\"] for items in qa_dataset])\n",
    "id2label = {idx : label for idx, label in enumerate(classes)}\n",
    "label2id = {label: id for id, label in id2label.items()}\n",
    "id2label, label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 23, 17]\n",
      "[5, 23, 17, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "PAD_IDX = vocab[\"<pad>\"]\n",
    "\n",
    "def pad_and_truncate(input_ids: list[int], max_seq_len: int):\n",
    "    if len(input_ids) > max_seq_len:\n",
    "        input_ids = input_ids[:max_seq_len]\n",
    "    else:\n",
    "        input_ids = input_ids + [PAD_IDX] * (max_seq_len - len(input_ids))\n",
    "    return input_ids\n",
    "MAX_SEQ_LEN = 5\n",
    "text = \"I love AIVN\"\n",
    "tokenized_text = tokenier(text)\n",
    "tokens = [vocab[token] for token in tokenized_text]\n",
    "print(tokens)\n",
    "tokens = pad_and_truncate(input_ids=tokens, max_seq_len=MAX_SEQ_LEN)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11,  6,  7, 14,  9,  1,  1,  1,  1,  1])\n",
      "tensor([ 7, 14,  6, 17, 12,  5, 10, 21, 31,  8,  1,  1,  1,  1,  1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "MAX_SEQ_LEN = 10\n",
    "MAX_CONTEXT_LEN = 15\n",
    "\n",
    "\n",
    "def vectorize(question: str, context: str):\n",
    "    input_ques_ids = [vocab[token] for token in tokenier(question)]\n",
    "    input_context_ids = [vocab[token] for token in tokenier(context)]\n",
    "\n",
    "    input_ques_ids = pad_and_truncate(input_ids=input_ques_ids, max_seq_len=MAX_SEQ_LEN)\n",
    "    input_context_ids = pad_and_truncate(input_ids=input_context_ids, max_seq_len=MAX_CONTEXT_LEN)\n",
    "\n",
    "    return (\n",
    "        torch.tensor(input_ques_ids, dtype=torch.long), \n",
    "        torch.tensor(input_context_ids, dtype=torch.long)\n",
    "    )\n",
    "input_ques_ids, input_context_ids = vectorize(\n",
    "    question=qa_dataset[0]['question'],\n",
    "    context=qa_dataset[0]['context']\n",
    ")\n",
    "\n",
    "print(input_ques_ids)\n",
    "print(input_context_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data: list[dict]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        question_text = item['question']\n",
    "        context_text = item['context']\n",
    "\n",
    "        input_ques_ids, input_context_ids = vectorize(\n",
    "            question=question_text,\n",
    "            context=context_text\n",
    "        )\n",
    "\n",
    "        answer_text = item['answer']\n",
    "        answer_id =torch.tensor(label2id[answer_text], dtype=torch.long)\n",
    "\n",
    "        return input_ques_ids, input_context_ids, answer_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11, 10,  5, 15,  9,  1,  1,  1,  1,  1]])\n",
      "tensor([[ 5, 10, 15, 20, 26, 19, 27, 29, 24, 28,  8,  1,  1,  1,  1]])\n",
      "tensor([2])\n",
      "====================================================================================================\n",
      "tensor([[11,  6,  7, 13, 16,  9,  1,  1,  1,  1]])\n",
      "tensor([[ 5, 23, 25, 12,  7, 13, 18,  6, 32, 30, 22,  8,  1,  1,  1]])\n",
      "tensor([0])\n",
      "====================================================================================================\n",
      "tensor([[11,  6,  7, 14,  9,  1,  1,  1,  1,  1]])\n",
      "tensor([[ 7, 14,  6, 17, 12,  5, 10, 21, 31,  8,  1,  1,  1,  1,  1]])\n",
      "tensor([1])\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataset = QADataset(data=qa_dataset)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              batch_size=1, \n",
    "                              shuffle=True)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    input_ques_ids, input_context_ids, answer_id = batch\n",
    "    print(input_ques_ids)\n",
    "    print(input_context_ids)\n",
    "    print(answer_id)\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, hidden_size: int, n_layers: int, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.embed_model = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        self.model = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_size, \n",
    "            batch_first=True, \n",
    "            bidirectional=True,\n",
    "            num_layers=n_layers,\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(in_features=hidden_size*2,\n",
    "                                    out_features=n_classes)\n",
    "    \n",
    "    def forward(self, question, context): \n",
    "        # question: [N, seq_ques], context: [N, seq_context]\n",
    "        question_embed = self.embed_model(question) # [N, seq_ques, embed_dim]\n",
    "        context_embed = self.embed_model(context) # [N, seq_context, embed_dim]\n",
    "\n",
    "        combined = torch.cat(\n",
    "            tensors=(question_embed, context_embed), dim=1\n",
    "        ) # [N, seq_ques + seq_context, embed_dim]\n",
    "        lstm_output, (hidden_lstm, cell_lstm) = self.model(combined)\n",
    "        # lstm_output: [N, seq_ques + seq_context, hidden_size * 2]\n",
    "        # hidden_lstm: [num_layers, N, hidden_size]\n",
    "        # cell_lstm: [num_layers, N, hidden_size]\n",
    "        lstm_output = lstm_output[:, -1, :]\n",
    "        \n",
    "        out = self.classifier(lstm_output)\n",
    "        return out\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_SIZE = 128\n",
    "VOCAB_SIZE = len(vocab)\n",
    "N_LAYERS  = 2\n",
    "N_CLASSES = len(classes)\n",
    "\n",
    "model =QAModel(vocab_size=VOCAB_SIZE, \n",
    "               embed_dim=EMBEDDING_DIM,\n",
    "               hidden_size=HIDDEN_SIZE,\n",
    "               n_layers=N_LAYERS,\n",
    "               n_classes= N_CLASSES)\n",
    "\n",
    "input_ques =torch.randint(low=0, high=1, size=(1, MAX_SEQ_LEN))\n",
    "input_context = torch.randint(low=0, high=1, size=(1, MAX_CONTEXT_LEN))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ques, input_context)\n",
    "    print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14091157913208\n",
      "1.1665637493133545\n",
      "1.238621711730957\n",
      "0.9888670444488525\n",
      "1.2195442914962769\n",
      "1.1216247081756592\n",
      "1.1719011068344116\n",
      "1.1135239601135254\n",
      "1.0356508493423462\n",
      "1.0421650409698486\n",
      "1.0850640535354614\n",
      "1.1281003952026367\n",
      "1.0375242233276367\n",
      "1.054024577140808\n",
      "1.1255751848220825\n",
      "1.021980881690979\n",
      "1.0402880907058716\n",
      "1.1116539239883423\n",
      "1.0889383554458618\n",
      "0.9556953310966492\n",
      "1.050256371498108\n",
      "0.9735899567604065\n",
      "0.8948920369148254\n",
      "1.044875144958496\n",
      "1.028849482536316\n",
      "0.7558454275131226\n",
      "0.7407063245773315\n",
      "0.9343756437301636\n",
      "0.6113422513008118\n",
      "0.4608971178531647\n",
      "0.8252317309379578\n",
      "0.30520883202552795\n",
      "0.3908999264240265\n",
      "0.3339079022407532\n",
      "0.142010897397995\n",
      "0.5078392624855042\n",
      "0.40806397795677185\n",
      "0.14799711108207703\n",
      "0.052975017577409744\n",
      "0.04200468957424164\n",
      "0.08921055495738983\n",
      "0.2290087640285492\n",
      "0.04931109771132469\n",
      "0.018480265513062477\n",
      "0.07119274139404297\n",
      "0.054729729890823364\n",
      "0.0223334189504385\n",
      "0.009882924146950245\n",
      "0.027761150151491165\n",
      "0.007564708590507507\n",
      "0.01261010579764843\n",
      "0.01116174552589655\n",
      "0.014487551525235176\n",
      "0.004785038530826569\n",
      "0.008006265386939049\n",
      "0.003950055688619614\n",
      "0.008966065011918545\n",
      "0.006109490990638733\n",
      "0.0030613720882683992\n",
      "0.006370830815285444\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "EPOCHS = 20\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for _ in range(EPOCHS):\n",
    "    for idx, (input_ques_ids, input_context_ids, answer_id) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs= model(input_ques_ids, input_context_ids)\n",
    "        loss = criterion(outputs, answer_id)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(loss.item())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: My name is AIVN and I am from Vietnam.\n",
      "Question: What is my name?\n",
      "Prediction: AIVN\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample = qa_dataset[0]\n",
    "    context, question, answer = sample.values()\n",
    "    question_ids, context_ids = vectorize(question=question, context=context)\n",
    "    question_ids = question_ids.unsqueeze(0) # add batch dimention\n",
    "    context_ids = context_ids.unsqueeze(0)\n",
    "\n",
    "    outputs = model(question_ids, context_ids)\n",
    "\n",
    "    _, predictions = torch.max(outputs.data, 1)\n",
    "    print(f\"Context: {context}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Prediction: {id2label[predictions.numpy()[0]]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
