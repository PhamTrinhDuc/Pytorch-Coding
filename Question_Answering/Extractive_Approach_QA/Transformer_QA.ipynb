{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset = [\n",
    "    {\n",
    "        'context': 'My name is AIVN and I am from Vietnam.',\n",
    "        'question': 'What is my name?',\n",
    "        'answer': 'AIVN'\n",
    "    },\n",
    "    {\n",
    "        'context': 'I love painting and my favorite artist is Vincent Van Gogh.',\n",
    "        'question': 'What is my favorite activity?',\n",
    "        'answer': 'painting'\n",
    "    },\n",
    "    {\n",
    "        'context': 'I am studying computer science at the University of Tokyo.',\n",
    "        'question': 'What am I studying?',\n",
    "        'answer': 'computer science'\n",
    "    },\n",
    "    {\n",
    "        'context': 'My favorite book is \"To Kill a Mockingbird\" by Harper Lee.',\n",
    "        'question': 'What is my favorite book?', \n",
    "        'answer': '\"To Kill a Mockingbird\"'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vincent': 41,\n",
       " 'vietnam': 40,\n",
       " 'university': 38,\n",
       " 'to': 36,\n",
       " 'the': 35,\n",
       " 'painting': 33,\n",
       " 'of': 32,\n",
       " 'mockingbird': 31,\n",
       " 'love': 30,\n",
       " 'lee': 29,\n",
       " 'what': 12,\n",
       " '<sep>': 4,\n",
       " '<bos>': 2,\n",
       " 'science': 34,\n",
       " '?': 9,\n",
       " 'my': 7,\n",
       " 'is': 6,\n",
       " 'at': 22,\n",
       " '<cls>': 5,\n",
       " 'gogh': 26,\n",
       " '.': 8,\n",
       " '<eos>': 3,\n",
       " '<pad>': 1,\n",
       " 'computer': 24,\n",
       " 'artist': 21,\n",
       " 'favorite': 10,\n",
       " 'harper': 27,\n",
       " '<unk>': 0,\n",
       " 'and': 14,\n",
       " 'studying': 17,\n",
       " 'i': 11,\n",
       " 'aivn': 20,\n",
       " 'am': 13,\n",
       " 'van': 39,\n",
       " 'book': 15,\n",
       " 'tokyo': 37,\n",
       " 'name': 16,\n",
       " 'kill': 28,\n",
       " 'by': 23,\n",
       " 'a': 18,\n",
       " 'from': 25,\n",
       " 'activity': 19}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "tokenier = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(examples: list):\n",
    "    for item in examples:\n",
    "        yield tokenier(\"<cls> \" + item[\"context\"] + \" <sep> \" + item[\"question\"])\n",
    "        # token cls dành cho các câu không có đáp án, ứng với st_pos = end_pos = 0\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    iterator=yield_tokens(qa_dataset), \n",
    "    specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\", \"<sep>\", \"<cls>\"]\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 30, 20]\n",
      "[11, 30, 20, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "PAD_IDX = vocab[\"<pad>\"]\n",
    "\n",
    "def pad_and_truncate(input_ids: list[int], max_seq_len: int):\n",
    "    if len(input_ids) > max_seq_len:\n",
    "        input_ids = input_ids[:max_seq_len]\n",
    "    else:\n",
    "        input_ids = input_ids + [PAD_IDX] * (max_seq_len - len(input_ids))\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "MAX_SEQ_LEN = 22\n",
    "text = \"I love AIVN\"\n",
    "tokenized_text = tokenier(text)\n",
    "tokens = [vocab[token] for token in tokenized_text]\n",
    "print(tokens)\n",
    "tokens = pad_and_truncate(input_ids=tokens, max_seq_len=MAX_SEQ_LEN)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12,  6,  7, 16,  9,  4,  7, 16,  6, 20, 14, 11, 13, 25, 40,  8,  1,  1,\n",
      "         1,  1,  1,  1])\n",
      "tensor(9)\n",
      "tensor(9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def vectorize(question: str, context: str, answer: str):\n",
    "    \n",
    "    input_text = question + \" <sep> \" + context\n",
    "    input_ids = [vocab[token] for token in tokenier(input_text)]\n",
    "    input_ids = pad_and_truncate(input_ids=input_ids, max_seq_len=MAX_SEQ_LEN)\n",
    "\n",
    "    answer_ids = [vocab[token] for token in tokenier(answer)]\n",
    "    st_pos = input_ids.index(answer_ids[0])\n",
    "    end_pos = st_pos + len(answer_ids) - 1\n",
    "\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    st_pos = torch.tensor(st_pos, dtype=torch.long)\n",
    "    end_pos = torch.tensor(end_pos, dtype=torch.long)\n",
    "    return input_ids, st_pos, end_pos\n",
    "\n",
    "input_ids, st_pos, end_pos = vectorize(\n",
    "    question=qa_dataset[0]['question'],\n",
    "    context=qa_dataset[0]['context'], \n",
    "    answer=qa_dataset[0]['answer']\n",
    ")\n",
    "print(input_ids)\n",
    "print(st_pos)\n",
    "print(end_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is my name ? <sep> my name is aivn and i am from vietnam . <pad> <pad> <pad> <pad> <pad> <pad> "
     ]
    }
   ],
   "source": [
    "id2token = {id: label for label, id in vocab.get_stoi().items()}\n",
    "for token in input_ids.numpy():\n",
    "    print(id2token[token], end= ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data: list[dict]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        question_text = item['question']\n",
    "        context_text = item['context']\n",
    "        answer_text = item['answer']\n",
    "\n",
    "        input_ids, st_pos, end_pos = vectorize(\n",
    "            question=question_text,\n",
    "            context=context_text,\n",
    "            answer=answer_text\n",
    "        )\n",
    "        return input_ids, st_pos, end_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12,  6,  7, 10, 19,  9,  4, 11, 30, 33, 14,  7, 10, 21,  6, 41, 39, 26,\n",
      "          8,  1,  1,  1]])\n",
      "tensor([9])\n",
      "tensor([9])\n",
      "====================================================================================================\n",
      "tensor([[12,  6,  7, 16,  9,  4,  7, 16,  6, 20, 14, 11, 13, 25, 40,  8,  1,  1,\n",
      "          1,  1,  1,  1]])\n",
      "tensor([9])\n",
      "tensor([9])\n",
      "====================================================================================================\n",
      "tensor([[12, 13, 11, 17,  9,  4, 11, 13, 17, 24, 34, 22, 35, 38, 32, 37,  8,  1,\n",
      "          1,  1,  1,  1]])\n",
      "tensor([9])\n",
      "tensor([10])\n",
      "====================================================================================================\n",
      "tensor([[12,  6,  7, 10, 15,  9,  4,  7, 10, 15,  6, 36, 28, 18, 31, 23, 27, 29,\n",
      "          8,  1,  1,  1]])\n",
      "tensor([11])\n",
      "tensor([14])\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataset = QADataset(data=qa_dataset)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              batch_size=1, \n",
    "                              shuffle=True)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    input_ids, st_pos, end_pos = batch\n",
    "    print(input_ids)\n",
    "    print(st_pos)\n",
    "    print(end_pos)\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embed_dim: int, \n",
    "                 num_heads: int, \n",
    "                 ff_dim: int,\n",
    "                 dropout_prob: float = 0.1):\n",
    "        super().__init__()\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads, # create num_heads attention\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(in_features=embed_dim, out_features=ff_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=ff_dim, out_features=embed_dim)\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = nn.LayerNorm(normalized_shape=embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(normalized_shape=embed_dim)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
    "    \n",
    "    def forward(self, query, key, value): \n",
    "        # query, key, value: [N, seq_len, embed_dim]\n",
    "        attn_output, attn_output_weights = self.attn(query, key, value)\n",
    "        # print(\"attn_output: \", attn_output.size()) # => output model same input: [N, seq_len, embed_dim]\n",
    "        # print(\"attn_output_weights: \", attn_output_weights.size()) => softmax(Q@K.T): [N, seq_len, seq_len]\n",
    "\n",
    "        attn_output = self.dropout1(attn_output) # [N, seq_len, embed_dim]\n",
    "        out_1 = self.layernorm1(query + attn_output) # [N, seq_len, embed_dim]\n",
    "        ffn_output = self.ffn(out_1) # [N, seq_len, embed_dim]\n",
    "        ffn_output = self.dropout2(ffn_output) # [N, seq_len, embed_dim]\n",
    "        out_2 = self.layernorm2(out_1 + ffn_output) # [N seq_len, embed_dim]\n",
    "        return out_2 # [N,seq_len, embedim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embed_dim: int, \n",
    "                 vocab_size: int, \n",
    "                 max_length: int):\n",
    "        super().__init__()\n",
    "        self.embed_model = nn.Embedding(\n",
    "            num_embeddings=vocab_size, \n",
    "            embedding_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        self.pos_embed = nn.Embedding(\n",
    "            num_embeddings=max_length,\n",
    "            embedding_dim=embed_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_len = x.size() # 32, 128\n",
    "        positions = torch.arange(0, seq_len).expand(N, seq_len) # [N, seq_len]\n",
    "        token_embed = self.embed_model(x) # [N, seq_len, embed_dim]\n",
    "        position_embed = self.pos_embed(positions) # [N, seq_len, embed_dim]\n",
    "        return token_embed + position_embed # [N, seq_len, embed_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size: int, \n",
    "                 embed_dim: int, \n",
    "                 n_heads: int, \n",
    "                 ff_dim: int,\n",
    "                 seq_len: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_model = TokenAndPositionEmbedding(embed_dim=embed_dim, \n",
    "                                                     vocab_size=vocab_size, \n",
    "                                                     max_length=seq_len)\n",
    "        self.transformer = TransformerBlock(embed_dim=embed_dim,\n",
    "                                            num_heads=n_heads,\n",
    "                                            ff_dim=ff_dim)\n",
    "        \n",
    "        self.st_linear = nn.Linear(in_features=embed_dim,\n",
    "                                   out_features=1)\n",
    "        self.end_linear = nn.Linear(in_features=embed_dim, \n",
    "                                    out_features=1)\n",
    "    def forward(self, input): # input: [N, seq_len]\n",
    "        embedding = self.embed_model(input) # [N, seq_len, embed_dim]\n",
    "        transformer_out = self.transformer(embedding, embedding, embedding) # [N, seq_len, embed_dim]\n",
    "        st_logits = self.st_linear(transformer_out).squeeze(-1) # [N, seq_len]\n",
    "        end_logits = self.end_linear(transformer_out).squeeze(-1) # [N, seq_len]\n",
    "        return st_logits, end_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 22])\n",
      "torch.Size([1, 22])\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 64\n",
    "FF_DIM = 128\n",
    "VOCAB_SIZE = len(vocab)\n",
    "N_HEADS  = 2\n",
    "\n",
    "\n",
    "model = QAModel(vocab_size=VOCAB_SIZE, \n",
    "                embed_dim=EMBEDDING_DIM,\n",
    "                n_heads=N_HEADS, \n",
    "                ff_dim=FF_DIM, \n",
    "                seq_len=MAX_SEQ_LEN)\n",
    "mock_data = torch.randint(low=0, high=10, size=(1, MAX_SEQ_LEN))\n",
    "\n",
    "output = model(mock_data)\n",
    "print(output[0].shape)\n",
    "print(output[1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.254849910736084\n",
      "3.3437860012054443\n",
      "3.2557053565979004\n",
      "3.444972038269043\n",
      "2.617384910583496\n",
      "2.982722282409668\n",
      "3.0575757026672363\n",
      "2.661027193069458\n",
      "2.593743324279785\n",
      "2.0395679473876953\n",
      "2.6776223182678223\n",
      "2.4687552452087402\n",
      "2.0641984939575195\n",
      "1.7262616157531738\n",
      "2.2824065685272217\n",
      "2.056992530822754\n",
      "1.9079394340515137\n",
      "1.2548178434371948\n",
      "1.7481648921966553\n",
      "1.4543797969818115\n",
      "1.541587471961975\n",
      "0.8851906061172485\n",
      "1.221709132194519\n",
      "1.0393528938293457\n",
      "0.8276312351226807\n",
      "0.8561490178108215\n",
      "0.45124197006225586\n",
      "0.8200867176055908\n",
      "0.6931801438331604\n",
      "0.33383098244667053\n",
      "0.5316500663757324\n",
      "0.4451179504394531\n",
      "0.2661314904689789\n",
      "0.397219181060791\n",
      "0.38671767711639404\n",
      "0.2752888798713684\n",
      "0.2576621174812317\n",
      "0.2204362154006958\n",
      "0.11286113411188126\n",
      "0.2247752845287323\n",
      "0.09133392572402954\n",
      "0.11935053765773773\n",
      "0.14225052297115326\n",
      "0.1153278797864914\n",
      "0.06740006804466248\n",
      "0.08350738883018494\n",
      "0.10964974015951157\n",
      "0.0869818702340126\n",
      "0.09620773792266846\n",
      "0.07457447797060013\n",
      "0.05715733766555786\n",
      "0.04342472925782204\n",
      "0.07098659873008728\n",
      "0.03938736021518707\n",
      "0.053780581802129745\n",
      "0.05326085537672043\n",
      "0.05354974791407585\n",
      "0.04215363785624504\n",
      "0.04987765848636627\n",
      "0.03758496791124344\n",
      "0.046556130051612854\n",
      "0.04668677598237991\n",
      "0.03507653623819351\n",
      "0.033475346863269806\n",
      "0.037506259977817535\n",
      "0.037672385573387146\n",
      "0.03416945040225983\n",
      "0.029406096786260605\n",
      "0.031742312014102936\n",
      "0.0317586287856102\n",
      "0.02370293438434601\n",
      "0.0318639874458313\n",
      "0.02859671041369438\n",
      "0.020996563136577606\n",
      "0.02643398568034172\n",
      "0.029321718961000443\n",
      "0.016574952751398087\n",
      "0.030245745554566383\n",
      "0.025172125548124313\n",
      "0.021557454019784927\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "EPOCHS = 20\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for _ in range(EPOCHS):\n",
    "    for idx, (input_ids, st_pos, end_pos) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        st_pos_logits, end_pos_logits = model(input_ids)\n",
    "        st_loss = criterion(st_pos_logits, st_pos)\n",
    "        end_loss = criterion(end_pos_logits, end_pos)\n",
    "        loss = (st_loss + end_loss) / 2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(loss.item())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: My favorite book is \"To Kill a Mockingbird\" by Harper Lee.\n",
      "Question: What is my favorite book?\n",
      "Prediction: to kill a mockingbird\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample = qa_dataset[3]\n",
    "    context, question, answer = sample.values()\n",
    "    input_ids, st_pos, end_pos = vectorize(question=question, \n",
    "                                           context=context, \n",
    "                                           answer=answer)\n",
    "    input_ids = input_ids.unsqueeze(0) # add batch dimention\n",
    "\n",
    "    st_logits, end_logits = model(input_ids)\n",
    "\n",
    "    offset = len(tokenier(question)) + 1\n",
    "    st_pos = torch.argmax(st_logits, dim=1).numpy()[0]\n",
    "    end_pos = torch.argmax(end_logits, dim=1).numpy()[0]\n",
    "    \n",
    "    st_pos -= offset\n",
    "    end_pos -= offset\n",
    "\n",
    "    st_pos = max(st_pos, 0)\n",
    "    end_pos = min(end_pos, len(tokenier(context)) -1) \n",
    "\n",
    "    if end_pos >= st_pos:\n",
    "        context_tokens = tokenier(context)\n",
    "        predicted_answer_tokens = context_tokens[st_pos: end_pos +1]\n",
    "        predicted_answer = \" \".join(predicted_answer_tokens)\n",
    "    else:\n",
    "        predicted_answer = \" \"\n",
    "    \n",
    "    print(f\"Context: {context}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Prediction: {predicted_answer}\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
