{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipeline](image.png)\n",
    "\n",
    "\n",
    "combine context + question -> model -> dự đoán ra 2 vị trí đầu và cuối của câu trả lời trong context "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset = [\n",
    "    {\n",
    "        'context': 'My name is AIVN and I am from Vietnam.',\n",
    "        'question': 'What is my name?',\n",
    "        'answer': 'AIVN'\n",
    "    },\n",
    "    {\n",
    "        'context': 'I love painting and my favorite artist is Vincent Van Gogh.',\n",
    "        'question': 'What is my favorite activity?',\n",
    "        'answer': 'painting'\n",
    "    },\n",
    "    {\n",
    "        'context': 'I am studying computer science at the University of Tokyo.',\n",
    "        'question': 'What am I studying?',\n",
    "        'answer': 'computer science'\n",
    "    },\n",
    "    {\n",
    "        'context': 'My favorite book is \"To Kill a Mockingbird\" by Harper Lee.',\n",
    "        'question': 'What is my favorite book?', \n",
    "        'answer': '\"To Kill a Mockingbird\"'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vincent': 40,\n",
       " 'vietnam': 39,\n",
       " 'university': 37,\n",
       " 'to': 35,\n",
       " 'the': 34,\n",
       " 'painting': 32,\n",
       " 'of': 31,\n",
       " 'mockingbird': 30,\n",
       " 'am': 12,\n",
       " 'what': 11,\n",
       " '<sep>': 4,\n",
       " '<bos>': 2,\n",
       " 'science': 33,\n",
       " '?': 8,\n",
       " 'my': 6,\n",
       " 'is': 5,\n",
       " 'at': 21,\n",
       " 'gogh': 25,\n",
       " 'love': 29,\n",
       " 'lee': 28,\n",
       " '.': 7,\n",
       " '<eos>': 3,\n",
       " '<pad>': 1,\n",
       " 'computer': 23,\n",
       " 'artist': 20,\n",
       " 'favorite': 9,\n",
       " 'harper': 26,\n",
       " '<unk>': 0,\n",
       " 'and': 13,\n",
       " 'studying': 16,\n",
       " 'i': 10,\n",
       " 'aivn': 19,\n",
       " 'van': 38,\n",
       " 'book': 14,\n",
       " 'tokyo': 36,\n",
       " 'name': 15,\n",
       " 'kill': 27,\n",
       " 'by': 22,\n",
       " 'a': 17,\n",
       " 'from': 24,\n",
       " 'activity': 18}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "tokenier = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(examples: list):\n",
    "    for item in examples:\n",
    "        yield tokenier(item[\"context\"] + \" <sep> \" + item[\"question\"])\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    iterator=yield_tokens(qa_dataset), \n",
    "    specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\", \"<sep>\"]\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 29, 19]\n",
      "[10, 29, 19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "PAD_IDX = vocab[\"<pad>\"]\n",
    "\n",
    "def pad_and_truncate(input_ids: list[int], max_seq_len: int):\n",
    "    if len(input_ids) > max_seq_len:\n",
    "        input_ids = input_ids[:max_seq_len]\n",
    "    else:\n",
    "        input_ids = input_ids + [PAD_IDX] * (max_seq_len - len(input_ids))\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "MAX_SEQ_LEN = 22\n",
    "text = \"I love AIVN\"\n",
    "tokenized_text = tokenier(text)\n",
    "tokens = [vocab[token] for token in tokenized_text]\n",
    "print(tokens)\n",
    "tokens = pad_and_truncate(input_ids=tokens, max_seq_len=MAX_SEQ_LEN)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11,  5,  6, 15,  8,  4,  6, 15,  5, 19, 13, 10, 12, 24, 39,  7,  1,  1,\n",
      "         1,  1,  1,  1])\n",
      "tensor(9)\n",
      "tensor(9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def vectorize(question: str, context: str, answer: str):\n",
    "    \n",
    "    input_text = question + \" <sep> \" + context\n",
    "    input_ids = [vocab[token] for token in tokenier(input_text)]\n",
    "    input_ids = pad_and_truncate(input_ids=input_ids, max_seq_len=MAX_SEQ_LEN)\n",
    "\n",
    "    answer_ids = [vocab[token] for token in tokenier(answer)]\n",
    "    st_pos = input_ids.index(answer_ids[0])\n",
    "    end_pos = st_pos + len(answer_ids) - 1\n",
    "\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    st_pos = torch.tensor(st_pos, dtype=torch.long)\n",
    "    end_pos = torch.tensor(end_pos, dtype=torch.long)\n",
    "    return input_ids, st_pos, end_pos\n",
    "\n",
    "input_ids, st_pos, end_pos = vectorize(\n",
    "    question=qa_dataset[0]['question'],\n",
    "    context=qa_dataset[0]['context'], \n",
    "    answer=qa_dataset[0]['answer']\n",
    ")\n",
    "print(input_ids)\n",
    "print(st_pos)\n",
    "print(end_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is my name ? <sep> my name is aivn and i am from vietnam . <pad> <pad> <pad> <pad> <pad> <pad> "
     ]
    }
   ],
   "source": [
    "id2token = {id: label for label, id in vocab.get_stoi().items()}\n",
    "for token in input_ids.numpy():\n",
    "    print(id2token[token], end= ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data: list[dict]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        question_text = item['question']\n",
    "        context_text = item['context']\n",
    "        answer_text = item['answer']\n",
    "\n",
    "        input_ids, st_pos, end_pos = vectorize(\n",
    "            question=question_text,\n",
    "            context=context_text,\n",
    "            answer=answer_text\n",
    "        )\n",
    "        return input_ids, st_pos, end_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11,  5,  6,  9, 14,  8,  4,  6,  9, 14,  5, 35, 27, 17, 30, 22, 26, 28,\n",
      "          7,  1,  1,  1]])\n",
      "tensor([11])\n",
      "tensor([14])\n",
      "====================================================================================================\n",
      "tensor([[11, 12, 10, 16,  8,  4, 10, 12, 16, 23, 33, 21, 34, 37, 31, 36,  7,  1,\n",
      "          1,  1,  1,  1]])\n",
      "tensor([9])\n",
      "tensor([10])\n",
      "====================================================================================================\n",
      "tensor([[11,  5,  6, 15,  8,  4,  6, 15,  5, 19, 13, 10, 12, 24, 39,  7,  1,  1,\n",
      "          1,  1,  1,  1]])\n",
      "tensor([9])\n",
      "tensor([9])\n",
      "====================================================================================================\n",
      "tensor([[11,  5,  6,  9, 18,  8,  4, 10, 29, 32, 13,  6,  9, 20,  5, 40, 38, 25,\n",
      "          7,  1,  1,  1]])\n",
      "tensor([9])\n",
      "tensor([9])\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataset = QADataset(data=qa_dataset)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              batch_size=1, \n",
    "                              shuffle=True)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    input_ids, st_pos, end_pos = batch\n",
    "    print(input_ids)\n",
    "    print(st_pos)\n",
    "    print(end_pos)\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size: int, \n",
    "                 embed_dim: int, \n",
    "                 hidden_size: int, \n",
    "                 n_layers: int, \n",
    "                 n_classes: int):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embed_model = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        self.model = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_size, \n",
    "            batch_first=True, \n",
    "            bidirectional=True,\n",
    "            num_layers=n_layers,\n",
    "        )\n",
    "\n",
    "        self.st_linear = nn.Linear(in_features=hidden_size*2,\n",
    "                                    out_features=n_classes)\n",
    "\n",
    "        self.end_linear = nn.Linear(in_features=hidden_size*2, \n",
    "                                    out_features=n_classes)\n",
    "        \n",
    "    def forward(self, input_text): \n",
    "        # input_text: [N, max_seq_len]\n",
    "        input_embed = self.embed_model(input_text) # [N, max_seq_len, embed_dim]\n",
    "\n",
    "        lstm_output, (hidden_lstm, cell_lstm) = self.model(input_embed)\n",
    "        # lstm_output: [N, max_seq_len, hidden_size * 2]\n",
    "        # hidden_lstm: [num_layers, N, hidden_size]\n",
    "        # cell_lstm: [num_layers, N, hidden_size]\n",
    "        st_logits = self.st_linear(lstm_output).squeeze(-1) # [N, max_seq_len, 1]\n",
    "        end_logits = self.end_linear(lstm_output).squeeze(-1) # [N, max_seq_len, 1]\n",
    "        \n",
    "        return st_logits, end_logits\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 22])\n",
      "torch.Size([1, 22])\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_SIZE = 128\n",
    "VOCAB_SIZE = len(vocab)\n",
    "N_LAYERS  = 2\n",
    "N_CLASSES = 1\n",
    "\n",
    "model =QAModel(vocab_size=VOCAB_SIZE, \n",
    "               embed_dim=EMBEDDING_DIM,\n",
    "               hidden_size=HIDDEN_SIZE,\n",
    "               n_layers=N_LAYERS,\n",
    "               n_classes=N_CLASSES)\n",
    "\n",
    "input_text =torch.randint(low=0, high=1, size=(1, MAX_SEQ_LEN))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    st_logits, end_logits = model(input_text)\n",
    "    print(st_logits.shape)\n",
    "    print(end_logits.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.082472324371338\n",
      "3.1034016609191895\n",
      "3.062800407409668\n",
      "3.010685920715332\n",
      "2.948223114013672\n",
      "2.8977251052856445\n",
      "2.8654823303222656\n",
      "2.7436347007751465\n",
      "2.7099392414093018\n",
      "2.582576274871826\n",
      "2.45509672164917\n",
      "2.4918534755706787\n",
      "2.2118425369262695\n",
      "2.0925960540771484\n",
      "1.881577968597412\n",
      "1.9772119522094727\n",
      "1.8046197891235352\n",
      "1.4406275749206543\n",
      "1.2628209590911865\n",
      "1.3313201665878296\n",
      "1.1346583366394043\n",
      "1.0563921928405762\n",
      "0.9186200499534607\n",
      "1.1796045303344727\n",
      "0.6512924432754517\n",
      "0.5703752040863037\n",
      "0.48697802424430847\n",
      "0.9063836336135864\n",
      "0.3084314465522766\n",
      "0.7481429576873779\n",
      "0.3281988203525543\n",
      "0.2846468985080719\n",
      "0.4591107964515686\n",
      "0.1299721747636795\n",
      "0.10012119263410568\n",
      "0.21611736714839935\n",
      "0.06005662679672241\n",
      "0.18729552626609802\n",
      "0.10631665587425232\n",
      "0.03636067733168602\n",
      "0.12047520279884338\n",
      "0.022033020853996277\n",
      "0.07097271084785461\n",
      "0.016704833135008812\n",
      "0.03928197920322418\n",
      "0.015969855710864067\n",
      "0.01482012216001749\n",
      "0.06606951355934143\n",
      "0.026252303272485733\n",
      "0.010089986026287079\n",
      "0.025559009984135628\n",
      "0.005860698409378529\n",
      "0.005344283767044544\n",
      "0.01802065595984459\n",
      "0.004406993743032217\n",
      "0.01763876900076866\n",
      "0.003930629231035709\n",
      "0.017453745007514954\n",
      "0.012842158786952496\n",
      "0.00270980061031878\n",
      "0.009699773043394089\n",
      "0.0023047560825943947\n",
      "0.0024540885351598263\n",
      "0.007016932126134634\n",
      "0.001852501998655498\n",
      "0.008838445879518986\n",
      "0.005850488319993019\n",
      "0.0022928083781152964\n",
      "0.002230252604931593\n",
      "0.005290820263326168\n",
      "0.008550284430384636\n",
      "0.0014140140265226364\n",
      "0.007526508532464504\n",
      "0.0013357363641262054\n",
      "0.0016426187939941883\n",
      "0.00426369346678257\n",
      "0.005603183060884476\n",
      "0.0012736571952700615\n",
      "0.0013817758299410343\n",
      "0.003879874013364315\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "EPOCHS = 20\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for _ in range(EPOCHS):\n",
    "    for idx, (input_ids, st_pos, end_pos) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        st_pos_logits, end_pos_logits = model(input_ids)\n",
    "        st_loss = criterion(st_pos_logits, st_pos)\n",
    "        end_loss = criterion(end_pos_logits, end_pos)\n",
    "        loss = (st_loss + end_loss) / 2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(loss.item())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample = qa_dataset[0]\n",
    "    context, question, answer = sample.values()\n",
    "    input_ids = vectorize(question=question, context=context, answer=answer)\n",
    "    input_ids = input_ids.unsqueeze(0) # add batch dimention\n",
    "\n",
    "    st_logits, end_logits = model(input_ids)\n",
    "\n",
    "    offset = len(tokenier(question)) + 2 \n",
    "    st_pos = torch.argmax(st_pos, dim=1).numpy()[0]\n",
    "    end_pos = torch.argmax(end_pos, dim=1).numpy()[0]\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Context: {context}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Prediction: {id2label[predictions.numpy()[0]]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
