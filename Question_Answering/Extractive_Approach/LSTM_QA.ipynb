{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pipeline](image.png)\n",
    "\n",
    "\n",
    "combine context + question -> model -> dự đoán ra 2 vị trí đầu và cuối của câu trả lời trong context "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset = [\n",
    "    {\n",
    "        'context': 'My name is AIVN and I am from Vietnam.',\n",
    "        'question': 'What is my name?',\n",
    "        'answer': 'AIVN'\n",
    "    },\n",
    "    {\n",
    "        'context': 'I love painting and my favorite artist is Vincent Van Gogh.',\n",
    "        'question': 'What is my favorite activity?',\n",
    "        'answer': 'painting'\n",
    "    },\n",
    "    {\n",
    "        'context': 'I am studying computer science at the University of Tokyo.',\n",
    "        'question': 'What am I studying?',\n",
    "        'answer': 'computer science'\n",
    "    },\n",
    "    {\n",
    "        'context': 'My favorite book is \"To Kill a Mockingbird\" by Harper Lee.',\n",
    "        'question': 'What is my favorite book?', \n",
    "        'answer': '\"To Kill a Mockingbird\"'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ducpham/anaconda3/lib/python3.11/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/ducpham/anaconda3/lib/python3.11/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/ducpham/anaconda3/lib/python3.11/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'vincent': 40,\n",
       " 'vietnam': 39,\n",
       " 'university': 37,\n",
       " 'to': 35,\n",
       " 'the': 34,\n",
       " 'painting': 32,\n",
       " 'of': 31,\n",
       " 'mockingbird': 30,\n",
       " 'am': 12,\n",
       " 'what': 11,\n",
       " '<sep>': 4,\n",
       " '<bos>': 2,\n",
       " 'science': 33,\n",
       " '?': 8,\n",
       " 'my': 6,\n",
       " 'is': 5,\n",
       " 'at': 21,\n",
       " 'gogh': 25,\n",
       " 'love': 29,\n",
       " 'lee': 28,\n",
       " '.': 7,\n",
       " '<eos>': 3,\n",
       " '<pad>': 1,\n",
       " 'computer': 23,\n",
       " 'artist': 20,\n",
       " 'favorite': 9,\n",
       " 'harper': 26,\n",
       " '<unk>': 0,\n",
       " 'and': 13,\n",
       " 'studying': 16,\n",
       " 'i': 10,\n",
       " 'aivn': 19,\n",
       " 'van': 38,\n",
       " 'book': 14,\n",
       " 'tokyo': 36,\n",
       " 'name': 15,\n",
       " 'kill': 27,\n",
       " 'by': 22,\n",
       " 'a': 17,\n",
       " 'from': 24,\n",
       " 'activity': 18}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "tokenier = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yield_tokens(examples: list):\n",
    "    for item in examples:\n",
    "        yield tokenier(item[\"context\"] + \" <sep> \" + item[\"question\"])\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    iterator=yield_tokens(qa_dataset), \n",
    "    specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\", \"<sep>\"]\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 29, 19]\n",
      "[10, 29, 19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "PAD_IDX = vocab[\"<pad>\"]\n",
    "\n",
    "def pad_and_truncate(input_ids: list[int], max_seq_len: int):\n",
    "    if len(input_ids) > max_seq_len:\n",
    "        input_ids = input_ids[:max_seq_len]\n",
    "    else:\n",
    "        input_ids = input_ids + [PAD_IDX] * (max_seq_len - len(input_ids))\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "MAX_SEQ_LEN = 22\n",
    "text = \"I love AIVN\"\n",
    "tokenized_text = tokenier(text)\n",
    "tokens = [vocab[token] for token in tokenized_text]\n",
    "print(tokens)\n",
    "tokens = pad_and_truncate(input_ids=tokens, max_seq_len=MAX_SEQ_LEN)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11,  5,  6, 15,  8,  4,  6, 15,  5, 19, 13, 10, 12, 24, 39,  7,  1,  1,\n",
      "         1,  1,  1,  1])\n",
      "tensor(9)\n",
      "tensor(9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def vectorize(question: str, context: str, answer: str):\n",
    "    \n",
    "    input_text = question + \" <sep> \" + context\n",
    "    input_ids = [vocab[token] for token in tokenier(input_text)]\n",
    "    input_ids = pad_and_truncate(input_ids=input_ids, max_seq_len=MAX_SEQ_LEN)\n",
    "\n",
    "    answer_ids = [vocab[token] for token in tokenier(answer)]\n",
    "    st_pos = input_ids.index(answer_ids[0])\n",
    "    end_pos = st_pos + len(answer_ids) - 1\n",
    "\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    st_pos = torch.tensor(st_pos, dtype=torch.long)\n",
    "    end_pos = torch.tensor(end_pos, dtype=torch.long)\n",
    "    return input_ids, st_pos, end_pos\n",
    "\n",
    "input_ids, st_pos, end_pos = vectorize(\n",
    "    question=qa_dataset[0]['question'],\n",
    "    context=qa_dataset[0]['context'], \n",
    "    answer=qa_dataset[0]['answer']\n",
    ")\n",
    "print(input_ids)\n",
    "print(st_pos)\n",
    "print(end_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is my name ? <sep> my name is aivn and i am from vietnam . <pad> <pad> <pad> <pad> <pad> <pad> "
     ]
    }
   ],
   "source": [
    "id2token = {id: label for label, id in vocab.get_stoi().items()}\n",
    "for token in input_ids.numpy():\n",
    "    print(id2token[token], end= ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, data: list[dict]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        question_text = item['question']\n",
    "        context_text = item['context']\n",
    "        answer_text = item['answer']\n",
    "\n",
    "        input_ids, st_pos, end_pos = vectorize(\n",
    "            question=question_text,\n",
    "            context=context_text,\n",
    "            answer=answer_text\n",
    "        )\n",
    "        return input_ids, st_pos, end_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11,  5,  6,  9, 14,  8,  4,  6,  9, 14,  5, 35, 27, 17, 30, 22, 26, 28,\n",
      "          7,  1,  1,  1]])\n",
      "tensor([11])\n",
      "tensor([14])\n",
      "====================================================================================================\n",
      "tensor([[11,  5,  6,  9, 18,  8,  4, 10, 29, 32, 13,  6,  9, 20,  5, 40, 38, 25,\n",
      "          7,  1,  1,  1]])\n",
      "tensor([9])\n",
      "tensor([9])\n",
      "====================================================================================================\n",
      "tensor([[11,  5,  6, 15,  8,  4,  6, 15,  5, 19, 13, 10, 12, 24, 39,  7,  1,  1,\n",
      "          1,  1,  1,  1]])\n",
      "tensor([9])\n",
      "tensor([9])\n",
      "====================================================================================================\n",
      "tensor([[11, 12, 10, 16,  8,  4, 10, 12, 16, 23, 33, 21, 34, 37, 31, 36,  7,  1,\n",
      "          1,  1,  1,  1]])\n",
      "tensor([9])\n",
      "tensor([10])\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataset = QADataset(data=qa_dataset)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                              batch_size=1, \n",
    "                              shuffle=True)\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    input_ids, st_pos, end_pos = batch\n",
    "    print(input_ids)\n",
    "    print(st_pos)\n",
    "    print(end_pos)\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class QAModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 vocab_size: int, \n",
    "                 embed_dim: int, \n",
    "                 hidden_size: int, \n",
    "                 n_layers: int, \n",
    "                 n_classes: int):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embed_model = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        self.model = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_size, \n",
    "            batch_first=True, \n",
    "            bidirectional=True,\n",
    "            num_layers=n_layers,\n",
    "        )\n",
    "\n",
    "        self.st_linear = nn.Linear(in_features=hidden_size*2,\n",
    "                                    out_features=n_classes)\n",
    "\n",
    "        self.end_linear = nn.Linear(in_features=hidden_size*2, \n",
    "                                    out_features=n_classes)\n",
    "        \n",
    "    def forward(self, input_text): \n",
    "        # input_text: [N, max_seq_len]\n",
    "        input_embed = self.embed_model(input_text) # [N, max_seq_len, embed_dim]\n",
    "\n",
    "        lstm_output, (hidden_lstm, cell_lstm) = self.model(input_embed)\n",
    "        # lstm_output: [N, max_seq_len, hidden_size * 2]\n",
    "        # hidden_lstm: [num_layers, N, hidden_size]\n",
    "        # cell_lstm: [num_layers, N, hidden_size]\n",
    "        st_logits = self.st_linear(lstm_output).squeeze(-1) # [N, max_seq_len, 1]\n",
    "        end_logits = self.end_linear(lstm_output).squeeze(-1) # [N, max_seq_len, 1]\n",
    "        \n",
    "        return st_logits, end_logits\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 22])\n",
      "torch.Size([1, 22])\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_SIZE = 128\n",
    "VOCAB_SIZE = len(vocab)\n",
    "N_LAYERS  = 2\n",
    "N_CLASSES = 1\n",
    "\n",
    "model =QAModel(vocab_size=VOCAB_SIZE, \n",
    "               embed_dim=EMBEDDING_DIM,\n",
    "               hidden_size=HIDDEN_SIZE,\n",
    "               n_layers=N_LAYERS,\n",
    "               n_classes=N_CLASSES)\n",
    "\n",
    "input_text =torch.randint(low=0, high=1, size=(1, MAX_SEQ_LEN))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    st_logits, end_logits = model(input_text)\n",
    "    print(st_logits.shape)\n",
    "    print(end_logits.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.089033603668213\n",
      "3.080734968185425\n",
      "3.047706127166748\n",
      "3.058602809906006\n",
      "2.968336582183838\n",
      "2.9694743156433105\n",
      "2.8675239086151123\n",
      "2.7737717628479004\n",
      "2.772329807281494\n",
      "2.646535873413086\n",
      "2.565298318862915\n",
      "2.569988250732422\n",
      "2.3456497192382812\n",
      "2.162170886993408\n",
      "2.086494207382202\n",
      "2.0802884101867676\n",
      "1.6717901229858398\n",
      "1.6423993110656738\n",
      "1.5897448062896729\n",
      "1.3732277154922485\n",
      "1.103492259979248\n",
      "1.2283656597137451\n",
      "1.1151273250579834\n",
      "0.8588740825653076\n",
      "0.6452548503875732\n",
      "1.058375597000122\n",
      "0.7677636742591858\n",
      "0.46890974044799805\n",
      "0.40237319469451904\n",
      "0.8452520370483398\n",
      "0.36408132314682007\n",
      "0.391229510307312\n",
      "0.2423102706670761\n",
      "0.2340986430644989\n",
      "0.695274829864502\n",
      "0.22322937846183777\n",
      "0.12938430905342102\n",
      "0.07479516416788101\n",
      "0.14029961824417114\n",
      "0.3061040937900543\n",
      "0.21380019187927246\n",
      "0.23403924703598022\n",
      "0.03682659566402435\n",
      "0.041764240711927414\n",
      "0.04405006766319275\n",
      "0.13051454722881317\n",
      "0.02148337848484516\n",
      "0.1496201604604721\n",
      "0.11250707507133484\n",
      "0.01291143149137497\n",
      "0.015408702194690704\n",
      "0.03128960356116295\n",
      "0.02618122473359108\n",
      "0.014796249568462372\n",
      "0.01735900342464447\n",
      "0.08799221366643906\n",
      "0.05872966721653938\n",
      "0.017794791609048843\n",
      "0.0070801349356770515\n",
      "0.006734459660947323\n",
      "0.015305663459002972\n",
      "0.023974454030394554\n",
      "0.004140185657888651\n",
      "0.005239009391516447\n",
      "0.024360548704862595\n",
      "0.013821244239807129\n",
      "0.004997129552066326\n",
      "0.003387667005881667\n",
      "0.012528626248240471\n",
      "0.023101404309272766\n",
      "0.0044771768152713776\n",
      "0.0029121297411620617\n",
      "0.010403705760836601\n",
      "0.0026532215997576714\n",
      "0.0037326188758015633\n",
      "0.0169517919421196\n",
      "0.0022967876866459846\n",
      "0.014813179150223732\n",
      "0.008088922128081322\n",
      "0.0029303310438990593\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "EPOCHS = 20\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for _ in range(EPOCHS):\n",
    "    for idx, (input_ids, st_pos, end_pos) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        st_pos_logits, end_pos_logits = model(input_ids)\n",
    "        st_loss = criterion(st_pos_logits, st_pos)\n",
    "        end_loss = criterion(end_pos_logits, end_pos)\n",
    "        loss = (st_loss + end_loss) / 2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(loss.item())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: My favorite book is \"To Kill a Mockingbird\" by Harper Lee.\n",
      "Question: What is my favorite book?\n",
      "Prediction: to kill a mockingbird\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample = qa_dataset[3]\n",
    "    context, question, answer = sample.values()\n",
    "    input_ids, st_pos, end_pos = vectorize(question=question, \n",
    "                                           context=context, \n",
    "                                           answer=answer)\n",
    "    input_ids = input_ids.unsqueeze(0) # add batch dimention\n",
    "\n",
    "    st_logits, end_logits = model(input_ids)\n",
    "\n",
    "    offset = len(tokenier(question)) + 1\n",
    "    st_pos = torch.argmax(st_logits, dim=1).numpy()[0]\n",
    "    end_pos = torch.argmax(end_logits, dim=1).numpy()[0]\n",
    "    \n",
    "    st_pos -= offset\n",
    "    end_pos -= offset\n",
    "\n",
    "    st_pos = max(st_pos, 0)\n",
    "    end_pos = min(end_pos, len(tokenier(context)) -1) \n",
    "\n",
    "    if end_pos >= st_pos:\n",
    "        context_tokens = tokenier(context)\n",
    "        predicted_answer_tokens = context_tokens[st_pos: end_pos +1]\n",
    "        predicted_answer = \" \".join(predicted_answer_tokens)\n",
    "    else:\n",
    "        predicted_answer = \" \"\n",
    "    \n",
    "    print(f\"Context: {context}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Prediction: {predicted_answer}\")\n",
    "    \n",
    "\n",
    "# LSTM QA extractive approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
