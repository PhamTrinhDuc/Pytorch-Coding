{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ducpham/Documents/Pytorch-Coding/Text_generation/Poem_Generation\n"
     ]
    }
   ],
   "source": [
    "cd ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ducpham/anaconda3/lib/python3.11/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/ducpham/anaconda3/lib/python3.11/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/ducpham/anaconda3/lib/python3.11/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'poem-datasets.csv'\n",
    "df = pd.read_csv(DATASET_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'][0].split('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Build vectorization function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normalize(text):\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def yield_tokens(df):\n",
    "    for idx, row in df.iterrows():\n",
    "        yield tokenizer(row['content'])\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(df),\n",
    "    specials=['<unk>', '<pad>', '<sos>', '<eos>', '<eol>']\n",
    ")\n",
    "\n",
    "\n",
    "df['content'] = df['content'].apply(lambda x: text_normalize(x))\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = vocab['<pad>']\n",
    "EOS_TOKEN = vocab['<eos>']\n",
    "\n",
    "MAX_SEQ_LEN = 25\n",
    "\n",
    "def pad_and_truncate(input_ids, max_seq_len):\n",
    "    if len(input_ids) > max_seq_len:\n",
    "        input_ids = input_ids[:max_seq_len]\n",
    "    else:\n",
    "        input_ids += [PAD_TOKEN] * (max_seq_len - len(input_ids))\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "def vectorize(text, max_seq_len):\n",
    "    input_ids = [vocab[token] for token in tokenizer(text)]\n",
    "    input_ids = pad_and_truncate(input_ids, max_seq_len)\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "def decode(input_ids):\n",
    "    return [vocab.get_itos()[token_id] for token_id in input_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bạn xấu như chiếc bóng\n",
      "[402, 1812, 34, 322, 68, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(df['content'][0].split('\\n')[0])\n",
    "print(vectorize(df['content'][0].split('\\n')[0], 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Create Poem Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, vectorizer, max_seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vectorizer = vectorizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.input_seqs, self.target_seqs, self.padding_masks = self.create_samples(df)\n",
    "\n",
    "    def create_padding_mask(self, input_ids, pad_token_id=PAD_TOKEN):\n",
    "        return [0 if token_id == pad_token_id else 1 for token_id in input_ids]\n",
    "\n",
    "    def split_content(self, content):\n",
    "        samples = []\n",
    "\n",
    "        poem_parts = content.split('\\n\\n')\n",
    "        for poem_part in poem_parts:\n",
    "            poem_in_lines = poem_part.split('\\n')\n",
    "            if len(poem_in_lines) == 4:\n",
    "                samples.append(poem_in_lines)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def prepare_sample(self, sample):\n",
    "        input_seqs = []\n",
    "        target_seqs = []\n",
    "        padding_masks = []\n",
    "\n",
    "        input_text = '<sos> ' + ' <eol> '.join(sample) + ' <eol> <eos>'\n",
    "        input_ids = self.tokenizer(input_text)\n",
    "        for idx in range(1, len(input_ids)):\n",
    "            input_seq = ' '.join(input_ids[:idx])\n",
    "            target_seq = ' '.join(input_ids[1:idx+1])\n",
    "            input_seq = self.vectorizer(input_seq, self.max_seq_len)\n",
    "            target_seq = self.vectorizer(target_seq, self.max_seq_len)\n",
    "            padding_mask = self.create_padding_mask(input_seq)\n",
    "\n",
    "            input_seqs.append(input_seq)\n",
    "            target_seqs.append(target_seq)\n",
    "            padding_masks.append(padding_mask)\n",
    "\n",
    "        return input_seqs, target_seqs, padding_masks\n",
    "\n",
    "    def create_samples(self, df):\n",
    "        input_seqs = []\n",
    "        target_words = []\n",
    "        padding_masks = []\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            content = row['content']\n",
    "            samples = self.split_content(content)\n",
    "            for sample in samples:\n",
    "                sample_input_seqs, sample_target_words, sample_padding_masks = self.prepare_sample(sample)\n",
    "\n",
    "                input_seqs += sample_input_seqs\n",
    "                target_words += sample_target_words\n",
    "                padding_masks += sample_padding_masks\n",
    "\n",
    "        input_seqs = torch.tensor(input_seqs, dtype=torch.long)\n",
    "        target_words = torch.tensor(target_words, dtype=torch.long)\n",
    "        padding_masks = torch.tensor(padding_masks, dtype=torch.float)\n",
    "\n",
    "        return input_seqs, target_words, padding_masks\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seqs = self.input_seqs[idx]\n",
    "        target_seqs = self.target_seqs[idx]\n",
    "        padding_masks = self.padding_masks[idx]\n",
    "\n",
    "        return input_seqs, target_seqs, padding_masks\n",
    "\n",
    "TRAIN_BS = 256\n",
    "train_dataset = PoemDataset(\n",
    "    df=df,\n",
    "    tokenizer=tokenizer,\n",
    "    vectorizer=vectorize,\n",
    "    max_seq_len=MAX_SEQ_LEN\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BS,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1])\n",
      "tensor([402,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1])\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "input_seqs, target_seqs, padding_masks = next(iter(train_loader))\n",
    "\n",
    "print(input_seqs[0])\n",
    "print(target_seqs[0])\n",
    "print(padding_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['Bạn', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "for idx in range(MAX_SEQ_LEN):\n",
    "    print(decode(input_seqs[idx]))\n",
    "    print(decode(target_seqs[idx]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Create model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dims, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dims, 2) * (-math.log(10000.0) / embedding_dims))\n",
    "        pe = torch.zeros(max_len, 1, embedding_dims)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dims: int,\n",
    "        n_heads: int,\n",
    "        hidden_dims: int,\n",
    "        n_layers: int,\n",
    "        dropout: float = 0.5\n",
    "    ):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, \n",
    "                                      embedding_dim=embedding_dims)\n",
    "        self.embedding_dims = embedding_dims\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dims=embedding_dims, \n",
    "                                              dropout=dropout)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dims,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=hidden_dims, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layers, \n",
    "            num_layers=n_layers\n",
    "        )\n",
    "        self.linear = nn.Linear(in_features=embedding_dims, \n",
    "                                out_features=vocab_size)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, \n",
    "                src: torch.Tensor, \n",
    "                src_mask: torch.Tensor=None, \n",
    "                padding_mask: torch.Tensor=None):\n",
    "        \n",
    "        # src: [N, seq_len]\n",
    "        N, seq_len = src.shape\n",
    "        src = self.embedding(src) * math.sqrt(self.embedding_dims)\n",
    "        src = self.pos_encoder(src)\n",
    "        if src_mask is None:\n",
    "            src_mask = torch.triu(input=torch.ones((seq_len, seq_len)), diagonal=1).bool()\n",
    "        output = self.transformer_encoder(src, mask=src_mask, src_key_padding_mask=padding_mask)\n",
    "        output = self.linear(output) # [N, seq_len, vocab_size]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 1924])\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocab) # 1924\n",
    "EMBEDDING_DIMS = 128\n",
    "HIDDEN_DIMS = 128\n",
    "MAX_SEQ_LEN = 25\n",
    "N_LAYERS = 2\n",
    "N_HEADS = 4\n",
    "DROPOUT = 0.2\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "input_tests = torch.randint(1, 10, (1, 10)).to(device)\n",
    "\n",
    "model = TransformerModel(\n",
    "    VOCAB_SIZE,\n",
    "    EMBEDDING_DIMS,\n",
    "    N_HEADS,\n",
    "    HIDDEN_DIMS,\n",
    "    N_LAYERS,\n",
    "    DROPOUT\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_tests)\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 5.0\n",
    "EPOCHS = 100\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 \t Loss 540.3614836193267\n",
      "EPOCH 2 \t Loss 426.22644478934154\n",
      "EPOCH 3 \t Loss 460.2862359909784\n",
      "EPOCH 4 \t Loss 341.24520510718935\n",
      "EPOCH 5 \t Loss 273.1403125581287\n",
      "EPOCH 6 \t Loss 231.195433480399\n",
      "EPOCH 7 \t Loss 278.8192298525856\n",
      "EPOCH 8 \t Loss 207.6133066813151\n",
      "EPOCH 9 \t Loss 191.31104678199404\n",
      "EPOCH 10 \t Loss 166.1860809326172\n",
      "EPOCH 11 \t Loss 183.60734194800966\n",
      "EPOCH 12 \t Loss 125.76590874081566\n",
      "EPOCH 13 \t Loss 113.23210089547294\n",
      "EPOCH 14 \t Loss 113.42255619594029\n",
      "EPOCH 15 \t Loss 86.52298118954613\n",
      "EPOCH 16 \t Loss 85.67820103963216\n",
      "EPOCH 17 \t Loss 85.51116380237397\n",
      "EPOCH 18 \t Loss 68.13210024152484\n",
      "EPOCH 19 \t Loss 78.66631153651646\n",
      "EPOCH 20 \t Loss 80.80233110700335\n",
      "EPOCH 21 \t Loss 49.53887131100609\n",
      "EPOCH 22 \t Loss 46.15447998046875\n",
      "EPOCH 23 \t Loss 44.43247259230841\n",
      "EPOCH 24 \t Loss 47.91933077857608\n",
      "EPOCH 25 \t Loss 45.615592684064595\n",
      "EPOCH 26 \t Loss 38.210173379807244\n",
      "EPOCH 27 \t Loss 34.63061977568127\n",
      "EPOCH 28 \t Loss 32.89529309953962\n",
      "EPOCH 29 \t Loss 34.86828095572336\n",
      "EPOCH 30 \t Loss 26.549080076671782\n",
      "EPOCH 31 \t Loss 22.312445322672527\n",
      "EPOCH 32 \t Loss 22.344249725341797\n",
      "EPOCH 33 \t Loss 15.45457953498477\n",
      "EPOCH 34 \t Loss 16.55784833998907\n",
      "EPOCH 35 \t Loss 14.935915992373513\n",
      "EPOCH 36 \t Loss 16.566687629336403\n",
      "EPOCH 37 \t Loss 11.725871858142671\n",
      "EPOCH 38 \t Loss 13.060111182076591\n",
      "EPOCH 39 \t Loss 10.894388425917853\n",
      "EPOCH 40 \t Loss 9.428010713486444\n",
      "EPOCH 41 \t Loss 8.004335130964007\n",
      "EPOCH 42 \t Loss 6.235864593869164\n",
      "EPOCH 43 \t Loss 5.934805166153681\n",
      "EPOCH 44 \t Loss 5.577221030280704\n",
      "EPOCH 45 \t Loss 6.448324612208775\n",
      "EPOCH 46 \t Loss 5.260924293881371\n",
      "EPOCH 47 \t Loss 4.902531374068487\n",
      "EPOCH 48 \t Loss 4.958356516701834\n",
      "EPOCH 49 \t Loss 4.6751606577918645\n",
      "EPOCH 50 \t Loss 4.688192344847179\n",
      "EPOCH 51 \t Loss 4.640939576285226\n",
      "EPOCH 52 \t Loss 4.632542087918236\n",
      "EPOCH 53 \t Loss 4.649329094659715\n",
      "EPOCH 54 \t Loss 4.8102212860470726\n",
      "EPOCH 55 \t Loss 4.815996442522321\n",
      "EPOCH 56 \t Loss 4.517063753945487\n",
      "EPOCH 57 \t Loss 4.499609424954369\n",
      "EPOCH 58 \t Loss 4.395044099716913\n",
      "EPOCH 59 \t Loss 4.265111151195708\n",
      "EPOCH 60 \t Loss 4.313853865578061\n",
      "EPOCH 61 \t Loss 4.298588230496361\n",
      "EPOCH 62 \t Loss 4.353345076243083\n",
      "EPOCH 63 \t Loss 4.348086243584042\n",
      "EPOCH 64 \t Loss 4.214197783243089\n",
      "EPOCH 65 \t Loss 4.448566209702265\n",
      "EPOCH 66 \t Loss 4.583480153764997\n",
      "EPOCH 67 \t Loss 4.428584144228981\n",
      "EPOCH 68 \t Loss 4.431752659025646\n",
      "EPOCH 69 \t Loss 4.352230208260672\n",
      "EPOCH 70 \t Loss 4.350919065021333\n",
      "EPOCH 71 \t Loss 4.286503700982957\n",
      "EPOCH 72 \t Loss 4.291485695611863\n",
      "EPOCH 73 \t Loss 4.277840035302298\n",
      "EPOCH 74 \t Loss 4.29187147957938\n",
      "EPOCH 75 \t Loss 4.395160833994548\n",
      "EPOCH 76 \t Loss 4.407035418919155\n",
      "EPOCH 77 \t Loss 4.2971029962812155\n",
      "EPOCH 78 \t Loss 4.225375493367513\n",
      "EPOCH 79 \t Loss 4.1986048221588135\n",
      "EPOCH 80 \t Loss 4.3398483367193315\n",
      "EPOCH 81 \t Loss 4.340870879945301\n",
      "EPOCH 82 \t Loss 4.337568101428804\n",
      "EPOCH 83 \t Loss 4.2097554206848145\n",
      "EPOCH 84 \t Loss 4.221897272836594\n",
      "EPOCH 85 \t Loss 4.222987924303327\n",
      "EPOCH 86 \t Loss 4.241780099414644\n",
      "EPOCH 87 \t Loss 4.168939885639009\n",
      "EPOCH 88 \t Loss 4.1423287732260565\n",
      "EPOCH 89 \t Loss 4.127524795986357\n",
      "EPOCH 90 \t Loss 4.131815354029338\n",
      "EPOCH 91 \t Loss 4.090995209557669\n",
      "EPOCH 92 \t Loss 4.068660486312139\n",
      "EPOCH 93 \t Loss 4.068878571192424\n",
      "EPOCH 94 \t Loss 4.044984261194865\n",
      "EPOCH 95 \t Loss 4.03224379675729\n",
      "EPOCH 96 \t Loss 4.0326585542588\n",
      "EPOCH 97 \t Loss 4.03592248190017\n",
      "EPOCH 98 \t Loss 4.014524187360491\n",
      "EPOCH 99 \t Loss 3.9998592876252674\n",
      "EPOCH 100 \t Loss 3.9940920557294572\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    losses = []\n",
    "    for idx, samples in enumerate(train_loader):\n",
    "        input_seqs, target_seqs, padding_masks = samples\n",
    "        input_seqs = input_seqs.to(device)\n",
    "        target_seqs = target_seqs.to(device)\n",
    "        padding_masks = padding_masks.to(device)\n",
    "\n",
    "        output = model(input_seqs, padding_mask=padding_masks)\n",
    "        output = output.permute(0, 2, 1) # [N, vocab_size, seq_len]\n",
    "        loss = criterion(output, target_seqs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    total_loss = sum(losses) / len(losses)\n",
    "    print(f'EPOCH {epoch+1} \\t Loss {total_loss}')\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temperature(logits, temperature=1.0):\n",
    "    if temperature != 1.0:\n",
    "        logits = logits / temperature\n",
    "\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "    sampled_index = torch.multinomial(probabilities, 1).item()\n",
    "\n",
    "    return sampled_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Anh đồi <pad> nụ <pad> điều, Và… nhà cúng đâu \n",
      " hỏi ròng Ngư Ta Chưa Một sao <pad> Đem <pad> Ngày bé <pad> nhìn... \n",
      " không \n",
      " <pad> <pad> có <pad> trên Gian đầu <pad> Tự <pad> <pad> <pad> điên <pad> Những - \n",
      " thể nguyện \n",
      " mẹ <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "temperature = 1.2\n",
    "input_text = '<sos> Anh'\n",
    "input_tokens = tokenizer(input_text)\n",
    "input_ids = [vocab[token] for token in input_tokens]\n",
    "eos_token_id = vocab['<eos>']\n",
    "generated_ids = input_ids.copy()\n",
    "MAX_GENERATION_LEN = 50\n",
    "\n",
    "\n",
    "for _ in range(MAX_GENERATION_LEN):\n",
    "    input_tensor = torch.tensor([generated_ids], \n",
    "                                dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "\n",
    "    last_token_logits = outputs[0, -1, :]\n",
    "    next_token_id = sample_with_temperature(last_token_logits, temperature)\n",
    "    generated_ids.append(next_token_id)\n",
    "\n",
    "    if next_token_id == eos_token_id:\n",
    "        break\n",
    "\n",
    "# Convert the generated tokens back to text\n",
    "generated_text = decode(generated_ids)\n",
    "generated_text = ' '.join(generated_text)\n",
    "generated_text = generated_text.replace('<sos>', '')\n",
    "lines = generated_text.split('<eol>')\n",
    "for line in lines:\n",
    "    print(''.join(line))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
