{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PREPARE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 15, 7)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "corpus = [\n",
    "    \"ăn quả nhớ kẻ trồng cây\",\n",
    "    \"có chí thì nên\"\n",
    "]\n",
    "\n",
    "data_size = len(corpus)\n",
    "\n",
    "\n",
    "# Define the max vocabulary size and sequence length\n",
    "vocab_size = 15\n",
    "sequence_length = 7\n",
    "data_size, vocab_size, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trồng': 13,\n",
       " '<unk>': 0,\n",
       " 'có': 7,\n",
       " '<pad>': 1,\n",
       " '<sos_topic2>': 3,\n",
       " 'thì': 12,\n",
       " 'ăn': 14,\n",
       " '<sos_topic1>': 2,\n",
       " '<eos>': 4,\n",
       " 'chí': 5,\n",
       " 'nên': 10,\n",
       " 'cây': 6,\n",
       " 'quả': 11,\n",
       " 'kẻ': 8,\n",
       " 'nhớ': 9}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# define tokenizer function\n",
    "tokenizer  = get_tokenizer(\"basic_english\")\n",
    "# Create a function to yield list of tokens\n",
    "def yield_tokens(examples):\n",
    "    for text in examples:\n",
    "        yield tokenizer(text)\n",
    "    \n",
    "# Create vocabulary\n",
    "vocab = build_vocab_from_iterator(iterator=yield_tokens(corpus), \n",
    "                                  specials=[\"<unk>\", \"<pad>\", \"<sos_topic1>\", \"<sos_topic2>\", \"<eos>\"],\n",
    "                                  max_tokens=vocab_size)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "vocab.get_stoi()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  ['<sos_topic1>', 'ăn', 'quả', 'nhớ', 'kẻ', 'trồng', 'cây']\n",
      "y:  ['ăn', 'quả', 'nhớ', 'kẻ', 'trồng', 'cây', '<eos>']\n",
      "====================================================================================================\n",
      "X:  ['<sos_topic2>', 'có', 'chí', 'thì', 'nên']\n",
      "y:  ['có', 'chí', 'thì', 'nên', '<eos>']\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "data_X = []\n",
    "data_y = []\n",
    "\n",
    "\n",
    "corpus[0] = '<sos_topic1> ' + corpus[0] + ' <eos>'\n",
    "corpus[1] = '<sos_topic2> ' + corpus[1] + ' <eos>'\n",
    "\n",
    "\n",
    "for vector in corpus:\n",
    "    vector = vector.split()\n",
    "    data_X.append(vector[:-1])\n",
    "    data_y.append(vector[1:])\n",
    "\n",
    "for x, y in zip(data_X, data_y):\n",
    "    print(\"X: \", x)\n",
    "    print(\"y: \", y)\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [2, 14, 11, 9, 8, 13, 6]\n",
      "y:  [14, 11, 9, 8, 13, 6, 4]\n",
      "====================================================================================================\n",
      "X:  [3, 7, 5, 12, 10, 1, 1]\n",
      "y:  [7, 5, 12, 10, 4, 1, 1]\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and numericalize your samples\n",
    "\n",
    "def vectorize(X, y, vocab, sequence_length):\n",
    "    X_ids = [vocab[token] for token in X][:sequence_length]\n",
    "    X_ids = X_ids + [vocab[\"<pad>\"]] * (sequence_length - len(X))\n",
    "\n",
    "    y_ids = [vocab[token] for token in y][:sequence_length]\n",
    "    y_ids = y_ids + [vocab[\"<pad>\"]] * (sequence_length - len(y))\n",
    "    \n",
    "    return X_ids, y_ids\n",
    "\n",
    "data_X_ids = []\n",
    "data_y_ids = []\n",
    "\n",
    "for X, y in zip(data_X, data_y):\n",
    "    X_ids, y_ids = vectorize(X, y, vocab, sequence_length)\n",
    "    data_X_ids.append(X_ids)\n",
    "    data_y_ids.append(y_ids)\n",
    "\n",
    "\n",
    "for X_ids, y_ids in zip(data_X_ids, data_y_ids):\n",
    "    print(\"X: \", X_ids)\n",
    "    print(\"y: \", y_ids)\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7])\n",
      "torch.Size([2, 7])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96282/2901662461.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_y_ids = torch.tensor(data_y_ids, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "data_X_ids = torch.tensor(data_X_ids, dtype=torch.long)\n",
    "print(data_X_ids.shape)\n",
    "\n",
    "data_y_ids = torch.tensor(data_y_ids, dtype=torch.long)\n",
    "print(data_y_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2, 14, 11,  9,  8, 13,  6],\n",
       "        [ 3,  7,  5, 12, 10,  1,  1]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_X_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.TRAINING WITH RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TG_RNN(\n",
      "  (embedding): Embedding(15, 8)\n",
      "  (model): RNN(8, 8, batch_first=True)\n",
      "  (linear): Linear(in_features=8, out_features=15, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 15, 7])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TG_RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, \n",
    "                                      embedding_dim=embed_dim)\n",
    "        \n",
    "        self.model = nn.RNN(input_size=embed_dim, hidden_size=embed_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(in_features=embed_dim, out_features=vocab_size)\n",
    "    \n",
    "    def forward(self, x): # shape_x: [N, sequence_len]\n",
    "        \n",
    "        x = self.embedding(x) # [N, sequence_len, embed_dim]\n",
    "        output_rnn, hidden_rnn = self.model(x) # ouput_rnn: [N, sequence_len, hidden_dim], hidden_rnn: [num_layers, N, hidden_dim]\n",
    "        x = self.linear(output_rnn)\n",
    "        return x.permute(0, 2, 1) # [N, vocab_size, sequence_len] -> match with CrossEntropyLoss in Pytorch [request shape: [N, num_classes, sequence_len]]\n",
    "\n",
    "\n",
    "model = TG_RNN(vocab_size=vocab_size, embed_dim=8)\n",
    "print(model)\n",
    "model(data_X_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8961081504821777\n",
      "2.550565004348755\n",
      "2.2825536727905273\n",
      "2.0461933612823486\n",
      "1.8105595111846924\n",
      "1.583443522453308\n",
      "1.3702161312103271\n",
      "1.1732375621795654\n",
      "0.9928973317146301\n",
      "0.8317399621009827\n",
      "0.694553017616272\n",
      "0.5818873643875122\n",
      "0.4905191957950592\n",
      "0.41640955209732056\n",
      "0.3557373881340027\n",
      "0.30517321825027466\n",
      "0.2622518837451935\n",
      "0.22549180686473846\n",
      "0.194116473197937\n",
      "0.16760122776031494\n",
      "0.1452823430299759\n",
      "0.12647469341754913\n",
      "0.11062461137771606\n",
      "0.0972549244761467\n",
      "0.08592425286769867\n",
      "0.07623473554849625\n",
      "0.0678509920835495\n",
      "0.06052184849977493\n",
      "0.05410873144865036\n",
      "0.04857390746474266\n",
      "0.043866924941539764\n",
      "0.03984333202242851\n",
      "0.036343902349472046\n",
      "0.03326483443379402\n",
      "0.030547330155968666\n",
      "0.02815130352973938\n",
      "0.026040833443403244\n",
      "0.024180758744478226\n",
      "0.02253827638924122\n",
      "0.021085234358906746\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.AdamW(params=model.parameters(), lr=0.05)\n",
    "\n",
    "for _ in range(40):\n",
    "    optim.zero_grad()\n",
    "    outputs = model(data_X_ids) # [N, vocab_size, sequence_length]\n",
    "    loss = criterion(outputs, data_y_ids)\n",
    "    print(loss.item())\n",
    "    loss.backward()\n",
    "    optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 15, 7])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1924, -0.7306,  0.0602, -1.0373, -1.0373, -2.6971, -1.7461],\n",
       "        [-4.2316,  3.4399,  1.2369, -0.7927, -1.0359,  2.2874,  0.7000],\n",
       "        [ 0.6310, -0.0917, -0.5092, -0.8957, -1.9059, -3.6407, -1.2883],\n",
       "        [ 0.5385, -0.2688,  0.3680, -0.6990, -1.1670, -2.2496, -0.8685],\n",
       "        [ 0.9083,  3.6521,  0.1939, -1.3532, -5.7020,  2.2908,  9.0254],\n",
       "        [-1.5692, -0.8419, -2.1892,  4.7700,  1.8199, -2.5830, -3.9032],\n",
       "        [-3.4662,  0.7420, -1.7625, -1.1869,  1.6814,  8.1480,  0.9097],\n",
       "        [ 0.1939, -0.5840,  0.4655, -2.2933, -1.4949,  2.5773,  3.6119],\n",
       "        [ 3.0281,  2.2016, -4.9895,  8.2871,  0.6823,  0.7786, -1.4901],\n",
       "        [ 2.6617, -5.0407,  7.8242, -4.2683,  4.3222, -3.2356, -0.5837],\n",
       "        [-2.1894, -3.4701,  2.3557, -5.1361,  0.3279, -0.1154,  2.2946],\n",
       "        [-1.0556,  7.9106, -5.8047,  2.5957, -7.5132,  1.6718,  4.1276],\n",
       "        [-1.5776, -3.3483, -1.8648, -0.1453,  5.6113,  4.3646, -2.6107],\n",
       "        [ 2.8453, -6.8360,  2.6319,  0.4222,  9.8748,  0.8032, -5.9898],\n",
       "        [ 8.2451, -0.6942,  2.2614,  2.5280,  3.7072, -0.7059, -0.0871]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1 từ tương ứng với 15 class được dự đoán -> lấy argmax theo chiều 15 (chiều 1)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1924, -4.2316,  0.6310,  0.5385,  0.9083, -1.5692, -3.4662,  0.1939,\n",
       "         3.0281,  2.6617, -2.1894, -1.0556, -1.5776,  2.8453,  8.2451],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].T[0] # ~ các class của từ thứ nhất được dự đoán "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.2451, grad_fn=<UnbindBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(outputs[0].T[0]) # ~ index 14 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14, 11,  9,  8, 13,  6,  4],\n",
       "        [ 7,  5, 12, 10,  4,  1,  1]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14, 11,  9,  8, 13,  6,  4],\n",
       "        [ 7,  5, 12, 10,  4,  1,  1]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 7, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "promt = '<sos_topic2> có'\n",
    "promt = promt.split()\n",
    "promt_ids = [vocab[token] for token in promt][:sequence_length]\n",
    "promt_ids = promt_ids + [vocab[\"<pad>\"]] * (sequence_length - len(promt))\n",
    "\n",
    "print(promt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 7, 5, 1, 1, 1, 1]\n",
      "['<sos_topic2>', 'có', 'chí', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "[3, 7, 5, 12, 1, 1, 1]\n",
      "['<sos_topic2>', 'có', 'chí', 'thì', '<pad>', '<pad>', '<pad>']\n",
      "[3, 7, 5, 12, 10, 1, 1]\n",
      "['<sos_topic2>', 'có', 'chí', 'thì', 'nên', '<pad>', '<pad>']\n",
      "[3, 7, 5, 12, 10, 4, 1]\n",
      "['<sos_topic2>', 'có', 'chí', 'thì', 'nên', '<eos>', '<pad>']\n",
      "[3, 7, 5, 12, 10, 4, 1]\n",
      "['<sos_topic2>', 'có', 'chí', 'thì', 'nên', '<eos>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "id2label = {id: label for label, id in vocab.get_stoi().items()}\n",
    "\n",
    "for i in range(sequence_length - len(promt)):\n",
    "    promt_tensor = torch.tensor(promt_ids, dtype=torch.long).reshape(1, -1)\n",
    "    outputs = model(promt_tensor)\n",
    "    outputs = torch.argmax(outputs, axis=1)   \n",
    "    next_id = outputs[0][len(promt)+i-1]\n",
    "\n",
    "    promt_ids[len(promt)+i] = next_id.item()\n",
    "    print(promt_ids)\n",
    "    prompt_token = [id2label[id] for id in promt_ids]\n",
    "    print(prompt_token) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
