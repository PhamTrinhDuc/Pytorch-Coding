{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"ăn quả nhớ kẻ trồng cây\",\n",
    "    \"có chí thì nên\"    \n",
    "]\n",
    "data_size = len(corpus)\n",
    "\n",
    "# Define the max vocabulary size and sequence length\n",
    "vocab_size = 15\n",
    "sequence_length = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trồng': 13,\n",
       " '<unk>': 0,\n",
       " 'có': 7,\n",
       " '<pad>': 1,\n",
       " '<sos_topic2>': 3,\n",
       " 'thì': 12,\n",
       " 'ăn': 14,\n",
       " '<sos_topic1>': 2,\n",
       " '<eos>': 4,\n",
       " 'chí': 5,\n",
       " 'nên': 10,\n",
       " 'cây': 6,\n",
       " 'quả': 11,\n",
       " 'kẻ': 8,\n",
       " 'nhớ': 9}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Define tokenizer function\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Create a function to yield list of tokens\n",
    "def yield_token(examples):\n",
    "    for text in examples:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = build_vocab_from_iterator(iterator=yield_token(corpus), \n",
    "                                  specials=[\"<unk>\", \"<pad>\", \"<sos_topic1>\", \"<sos_topic2>\", \"<eos>\"], \n",
    "                                  max_tokens=vocab_size)\n",
    "\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "vocab.get_stoi()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<sos_topic1>', 'ăn', 'quả', 'nhớ', 'kẻ', 'trồng', 'cây'], ['<sos_topic2>', 'có', 'chí', 'thì', 'nên']]\n",
      "[['ăn', 'quả', 'nhớ', 'kẻ', 'trồng', 'cây', '<eos>'], ['có', 'chí', 'thì', 'nên', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "data_X, data_y = [], []\n",
    "\n",
    "\n",
    "corpus[0] =  \"<sos_topic1> \" + corpus[0] + \" <eos>\"\n",
    "corpus[1] =  \"<sos_topic2> \" + corpus[1] + \" <eos>\" \n",
    "\n",
    "for vector in corpus:\n",
    "    vector = vector.split()\n",
    "    data_X.append(vector[:-1])\n",
    "    data_y.append(vector[1:])\n",
    "\n",
    "print(data_X)\n",
    "print(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and numericalize your samples\n",
    "def vectorize(X, y, vocab, sequence_length):\n",
    "    X_ids = [vocab[token] for token in X][:sequence_length]\n",
    "    y_ids = [vocab[token] for token in y][:sequence_length]\n",
    "\n",
    "    X_ids_pad = X_ids + [vocab[\"<pad>\"]] * (sequence_length - len(X))\n",
    "    y_ids_pad = y_ids + [vocab[\"<pad>\"]] * (sequence_length - len(y))\n",
    "\n",
    "    return X_ids_pad, y_ids_pad\n",
    "\n",
    "data_X_ids, data_y_ids =[], []\n",
    "\n",
    "for X, y in zip(data_X, data_y):\n",
    "    X_ids, y_ids = vectorize(X, y, vocab, sequence_length)\n",
    "    data_X_ids.append(X_ids)\n",
    "    data_y_ids.append(y_ids)\n",
    "\n",
    "data_X_ids = torch.tensor(data_X_ids, dtype=torch.long)\n",
    "data_y_ids = torch.tensor(data_y_ids, dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2, 14, 11,  9,  8, 13,  6])\n",
      "tensor([14, 11,  9,  8, 13,  6,  4])\n",
      "\n",
      "tensor([ 3,  7,  5, 12, 10,  1,  1])\n",
      "tensor([ 7,  5, 12, 10,  4,  1,  1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(data_X_ids, data_y_ids):\n",
    "    print(x)\n",
    "    print(y)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train with Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TG_Model(\n",
      "  (embedding): Embedding(15, 8)\n",
      "  (decoder_transformer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (linear2): Linear(in_features=4, out_features=8, bias=True)\n",
      "    (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.0, inplace=False)\n",
      "    (dropout2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (linear): Linear(in_features=8, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TG_Model(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "\n",
    "        self.mask = torch.triu(input=torch.ones(sequence_length, sequence_length), diagonal=1).bool()\n",
    "        self.decoder_transformer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=4, \n",
    "            dropout=0.0, \n",
    "            activation=\"relu\", \n",
    "            batch_first=True, \n",
    "            bias=True\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x): # shape x: [N, sequence_length]\n",
    "        embedding = self.embedding(x) # shape: [N, sequence_length, embed_dim]\n",
    "        output = self.decoder_transformer(embedding, src_mask=self.mask) # shape: [N, sequence_length, embed_dim]\n",
    "        output = self.linear(output) # shape: [N, sequence_length, vocab_size]\n",
    "\n",
    "        return output.permute(0, 2, 1) # shape: [N, vocab_size, sequence_length]\n",
    "\n",
    "model = TG_Model(vocab_size=vocab_size, embed_dim=8, num_heads=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 15, 7])\n"
     ]
    }
   ],
   "source": [
    "mock_data = data_X_ids\n",
    "output = model(mock_data)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9093026518821716\n",
      "0.7072864174842834\n",
      "0.5677958130836487\n",
      "0.35140368342399597\n",
      "0.2591227889060974\n",
      "0.1747504621744156\n",
      "0.11587797105312347\n",
      "0.07337851077318192\n",
      "0.047472644597291946\n",
      "0.032829366624355316\n",
      "0.023836275562644005\n",
      "0.017551401630043983\n",
      "0.012809100560843945\n",
      "0.009483846835792065\n",
      "0.007389824837446213\n",
      "0.0060529462061822414\n",
      "0.0048980629071593285\n",
      "0.003810920985415578\n",
      "0.0029855084139853716\n",
      "0.002408376196399331\n",
      "0.0019975665491074324\n",
      "0.0016854798886924982\n",
      "0.001431369804777205\n",
      "0.0012159096077084541\n",
      "0.0010354847181588411\n",
      "0.0008896319195628166\n",
      "0.0007741297013126314\n",
      "0.0006835010135546327\n",
      "0.0006120220059528947\n",
      "0.0005548485787585378\n",
      "0.0005084694712422788\n",
      "0.00047018862096592784\n",
      "0.0004379051097203046\n",
      "0.000410138803999871\n",
      "0.00038569868775084615\n",
      "0.00036380221717990935\n",
      "0.0003439132997300476\n",
      "0.00032570032635703683\n",
      "0.0003090443497058004\n",
      "0.0002938518300652504\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=0.05)\n",
    "\n",
    "for _ in range(40):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data_X_ids)\n",
    "    loss = criterion(output, data_y_ids)\n",
    "    print(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 11,  9,  8, 13,  6,  4],\n",
      "        [ 7,  5, 12, 10,  4,  1,  1]])\n"
     ]
    }
   ],
   "source": [
    "output = model(data_X_ids)\n",
    "print(torch.argmax(output, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[14, 11,  9,  8, 13,  6,  4],\n",
       "        [ 7,  5, 12, 10,  4,  1,  1]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 7, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "promt = '<sos_topic2> có'\n",
    "promt = promt.split()\n",
    "promt_ids = [vocab[token] for token in promt][:sequence_length]\n",
    "promt_ids = promt_ids + [vocab[\"<pad>\"]] * (sequence_length - len(promt))\n",
    "\n",
    "print(promt_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 7, 5, 12, 10, 4, 6]\n",
      "['<sos_topic2>', 'có', 'chí', 'thì', 'nên', '<eos>', 'cây']\n",
      "[3, 7, 5, 12, 10, 4, 6]\n",
      "['<sos_topic2>', 'có', 'chí', 'thì', 'nên', '<eos>', 'cây']\n",
      "[3, 7, 5, 12, 10, 4, 6]\n",
      "['<sos_topic2>', 'có', 'chí', 'thì', 'nên', '<eos>', 'cây']\n",
      "[3, 7, 5, 12, 10, 4, 6]\n",
      "['<sos_topic2>', 'có', 'chí', 'thì', 'nên', '<eos>', 'cây']\n",
      "[3, 7, 5, 12, 10, 4, 6]\n",
      "['<sos_topic2>', 'có', 'chí', 'thì', 'nên', '<eos>', 'cây']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "id2label = {id: label for label, id in vocab.get_stoi().items()}\n",
    "\n",
    "for i in range(sequence_length - len(promt)):\n",
    "    promt_tensor = torch.tensor(promt_ids, dtype=torch.long).reshape(1, -1)\n",
    "    outputs = model(promt_tensor)\n",
    "    outputs = torch.argmax(outputs, axis=1)   \n",
    "    next_id = outputs[0][len(promt)+i-1]\n",
    "\n",
    "    promt_ids[len(promt)+i] = next_id.item()\n",
    "    prompt_token = [id2label[id] for id in promt_ids]\n",
    "    print(promt_ids)\n",
    "    print(prompt_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
